PubSub Throughput Performance Tool
----------------------------------

Performance Metric for Publishers
  * Start timer
  * Publish an event, wait for ACK, then send the next event
  * A "publication" is a HTTP request/ACK pair
  * Timer is stopped after last ACK is received
  * Each publisher thread prints out total time taken
    to accept and acknowledge publications
  * Metric: rate of publications/sec that the PubSub Server acknowledged
    (that is, time to make back-to-back publications)

Performance Metric for Subscribers
  * Start timer when first event is received
  * Stop timer when last event is received
  * Each subscriber thread prints out time taken to receive all events
  * Metric: average delay between event deliveries to each subscriber

Architecture
  * All HTTP traffic via libcurl <http://curl.sourceforge.net/>
  * Subscriber app written in C 
  * Publisher app written in Python
  * Ancillary tools written in Python

Timing and Test Mechanics - Subscriber
  * Main program spawns the number of threads specified on the command line
  * Each thread is told how many events to expect
  * Each thread connects to a journal that is specified through the command line
  * Threads wait for a tag signifying that a performance test event was received
    * If first event, record the time
    * If last event, record the time

Setup
  * Install libcurl <http://curl.sourceforge.net/>
  * Install python <http://www.python.org/>
  * Install pycurl <http://pycurl.sourceforge.net/>
  * Build Source:  make -f Makefile.throughput_sub

How To Run The Tests
  * On client and server machines, be sure to have enough file descriptors
    * sudo bash
    * ulimit -n 65535   # Needs to be larger than numpubs + numsubs
    * You may have to configure your kernel too
      (for example, "echo 65535 > /proc/sys/fs/file-max" on Linux)
  * Run the PubSub Server in the new environment
  * Stop the PubSub Server, delete the event pool,
    and restart PubSub Server before all tests
  * Set the environment on the client machine by running "source testenv.bash"

Overview of Publisher - test run variables:
  * URL sequencing should be used; it instructs the publisher program to have
    each thread write to its own topic
  * Delay between posts
  * Number of events to be published by each thread
  * Output file
  * Size of events
  * Number of threads
  * Publish to URL
  * Verbose mode

makeroutes.py
  * Makes routes to journals from the publishers topics
  * Detects the number of publishers, assumes that they are run under URL sequencing
    and makes routes from each publisher's publish topic to that subscriber
    * -numpublishers - defines number of publisher threads to run
  * Uses environment variable to know which PubSub Server to use:
    * PUBTOPICURI - publish topic top level URI
    * SUBTOPICURI - subscribe topic top level URI
  * Uses a config file to read run parameters
  * Works in sequences
    * -startseq : start of sequence
    * -endseq : end of sequence
  * Creates the shell script that will be used to run the subscribers -
    the limitation here is that there can only be 1000 subscribers per publisher

makeroutes.py Overview: The "Card Dealer" Approach
  * makeroutes.py divvies up the total number of subscribers against the
    total number of publishers
  * For example, if there are 2 publishers and 2000 subscribers,
    * Subscribers 1 through 999 subscribe to journals attached to Publisher 1's topic
    * Subscribers 1000 through 1999 subscribe to journals attached to Publisher 2's topic

throughput_sub
  * Creates virtual subscribers
  * Urls created from sequences, input from files, or from command line;
    sequence mode is most heavily used and tested
  * Will discuss only sequence mode here; read code for additional details

throughput_sub options
  -t : Number of subscribers to simulate 
  -n : Number of events to expect
  -o : Output file to log to
  -seq : use sequence mode
  -startseq : first url number of sequence
  -endseq : last url number of sequence
  -urlbase: base url to subscribe to

Disclaimers
  * Still very raw!
  * Not tested at all on Windows
  * Publisher statistics are not collected automatically now
  * Lots of extra text output to publisher screen
  * Making routes is a pain

analyze.py to Analyze Data
  * If more than one instance of throughput_sub used,
    cat all results to one results file
  * Usage: python analyze.py -f <file name>

bash runall.bash config.ini
  * Validate that environment variables are set and ulimit is set;
    accept the config file name on the command line
  * makeroutes.py with appropriate config file
  * Start the subscribers from the shell program generated by makeroutes.py
  * Start the publisher(s)
  * Analyze the output after the publishers and subscribers complete
  * Refer to index.html for instructions in using this script and the testing tool

Sample Config File Parameters
  -numpublishers 50
  -numsubscribers 500
  -delaybetweenpublish 0
  -eventsize 512
  -eventsperpublisher 100
  -v 0 (verbosity level)
  -c 1 (URL sequencing is used)
  -puboutputfile 500-50-pub-timing.out
  -startseq 0
  -endseq 499
